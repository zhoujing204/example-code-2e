{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19: Concurrency Models in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Concurrency is about dealing with lots of things at once.\n",
    "> \n",
    "> Parallelism is about doing lots of things at once.\n",
    "> \n",
    "> Not the same, but related.\n",
    "> \n",
    "> One is about structure, one is about execution.\n",
    "> \n",
    "> Concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.\n",
    "> \n",
    "> -- Rob Pike, co-creator of Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bit of Jargon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concurrency: The ability to handle multiple pending tasks, making progress one at a time or in parallel(if possible) so that each of them eventually succeeds or fails. A single-core CPU is capable of concurrency if it runs an OS scheduler that interleaves the execution of different tasks. Also known as _multi-tasking_.\n",
    "\n",
    "- Parallelism: The ability to run multiple tasks _at the same time_. This requires a multi-core CPU, multiple CPUs, a GPU, or a cluster of computers. Parallelism is about _speeding up_ computation by using extra computing resources.\n",
    "\n",
    "- Execution unit: General term for objects that execute code concurrently, each with independent state and call stack. Python natively supports three kinds of execution units: processes, threads, and coroutines.\n",
    "\n",
    "- Process: An instance of a computer program while it is running, using memory and a slice of the CPU time. Modern desktop operating systems routinely manage hundreds of processes concurrently, with each process isolated in its own private memory space. Processes communicate via pipes, sockets, or memory mapped files -- all of which can only carry raw bytes. Python objects must be serialized(converted) into raw bytes to pass from one process to another. This is costly, and not all Python objects are serializable. A process can spawn subprocesses, each called a child process. These are also isolated from each other and from the parent. Processes allow _preemptive multitasking_: the OS scheduler _preempts_ --i.e., suspends -- each running process periodically to allow other processes to run. This means that a frozen process can't freeze the whole system -- in theory.\n",
    "\n",
    "- Thread: An execution unit within a single process. When a process starts, it uses a single thread: the _main thread_. A process can create more threads to operate concurrently by calling operating system APIs. Threads within a process share the same memory space, which holds live Python objects. This allows easy data sharing between threads, but can also lead to corrupted data when more than one thread updates the same object concurrently. Like processes, threads also enable _preemptive multitasking_ under the supervision of the OS scheduler. A thread consumes less resources than a process doing the same job.\n",
    "\n",
    "- Coroutine: A function that can suspend itself and resume later. In Python, _classic coroutines_ are built from generator functions, and _native coroutines_ are defined with _async def_. \"Classic Coroutines\" on page 641 introduced the concept, and Chapter 21 covers the use of the supervision of an _event loop_, also in the same thread. Asynchronous programming frameworks such as _asyncio_, _Curio_, or _Trio_ provide an event loop and supporting libraries for nonblocking, coroutine-based I/O. Coroutines support _cooperative multitasking_: each coroutine must explicitly cede control with the _yield_ or _await_ keywords, so that another may proceed concurrently(but not in parallel). This means that any blocking code in coroutine blocks the execution of the event loop and all other coroutines--in contrast with the _preemptive multitasking_ of processes and threads. On the other hand, each coroutine consumes less resources that a thread doing the same job.\n",
    "\n",
    "- Queue: A data structure that lets us put and get items, usually in FIFO order: first in, first out. Queues allow separate execution units to exchange application data and control messages, such as error codes and signals to terminate. The implementation of a queue varies according to the underlying concurrency model: the _queue_ package in Python's standard library provides queue classes to support threads, while the _multiprocessing_ and _asyncio_ packages implement their own queue classes. The _queue_ and _asyncio_ packages also include queues that are not FIFO: _PriorityQueue_ and _LifoQueue_.\n",
    "\n",
    "- Lock: An object that execution units can use to synchronize their actions and avoid corrupting data. While updating a shared data structure, the running code should hold an associated lock. This signals other parts of the program to wait until the lock is released before accessing the same structure. The simplest type of lock is also known as a mutex(for mutual exclusion). The implementation of a lock depends on the underlying concurrency model.\n",
    "\n",
    "- Contention: Dispute over a limited asset. Resource contention happens when multiple execution units try to access a shared resource--such as a lock or storage. There's also CPU contention, when compute-intensive processes or threads must wait for the OS scheduler to give them CPU time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes, Threads, and Python's infamous GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how concepts we just saw apply to Python programming, in 10 points:\n",
    "\n",
    "1. Each instance of the Python interpreter is a process. You can start additional Python processes using `multiprocessing` or `concurrent.futures` libraries. Python `subprocess` library is designed to launch processes to run external programs, regardless of the languages used to write them.\n",
    "\n",
    "2. The Python interpreter uses a single thread to run the user's program and the memory garbage collector. You can start additional Python threads using the `threading` or `concurrent.futures` libraries.\n",
    "\n",
    "3. Access to object reference counts and other internal interpreter state is controlled by a lock, the Global Interpreter Lock(GIL). Only one Python thread can hold the GIL at any time. This means that only one thread can execute Python code at any time, regardless of the number of CPU cores.\n",
    "\n",
    "4. To prevent a Python thread from holding the GIL indefinitely, Python's bytecode interpreter pauses the current Python thread every 5ms by default, releasing the GIL. The thread can then try to reacquire the GIL, but if there are other threads waiting for it, the OS scheduler may pick one of them to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n",
      "0.008\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Call sys.getswitchinterval() to get the current value of the switch interval\n",
    "print(sys.getswitchinterval())\n",
    "\n",
    "# Change it with sys.setswitchinterval()\n",
    "sys.setswitchinterval(0.008)\n",
    "print(sys.setswitchinterval())\n",
    "sys.setswitchinterval(0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. When we write Python code, we have no control over the GIL. But a built-in function or an extension written in C--or any language that interfaces at the Python/C API level--can release the GIL while running time-consuming tasks.\n",
    "\n",
    "6. Every Python standard library function that makes a syscall release GIL. This includes all functions that perform disk I/O, network I/O, and `time.sleep()`. Many CPU-intensive function in the `Numpy/SciPy` libraries, as well as the compressing/decompressing functions from `zlib` and `bz2` modules also release the GIL.\n",
    "\n",
    "7. Extensions that integrate the Python/C API level can also launch other non Python threads that are not affected by the GIL. Such GIL-free threads generally cannot change Python objects, but they can read from and write to the memory underlying objects that support the buffer protocol, such as `bytearray`, `array.array`, and `NumPy` arrays.\n",
    "\n",
    "8. The effect of the GIL on network programming with Python threads is relatively small, because the I/O functions release the GIL, and reading or writing to the network always implies high latency--compared to reading and writing to memory. Consequently, each individual thread spends a lot of time waiting anyway, so their execution can be interleaved without major impact on the overall throughput. That's why David Beazley says: \"Python threads are great at doing nothing.\"\n",
    "\n",
    "9. Contention over GIL slows down compute-intensive Python threads. Sequential, single-threaded code is simpler and faster for such tasks.\n",
    "\n",
    "10. To run CPU-intensive Python code on multiple cores, you must use multiple Python processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is good summary of the `threading` module documentation:\n",
    "\n",
    "> CPython implementation detail: In CPython, due to the Global Interpreter Lock, only one thread can execute Python code at once (even though certain performance-oriented libraries might overcome this limitation). If you want your application to make better use of the computational resources of multi-core machines, you are advised to use `multiprocessing` or `concurrent.futures.ProcessPoolExecutor`. However, threading is still an appropriate model if you want to run multiple I/O-bound tasks simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best practice is that one thread runs the event loop and all coroutines, while additional threads carry out specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Concurrent Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spinner with Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the next few examples is simple: start a function that blocks for 3 seconds while animating characters in the terminal to let the user know the program is \"thinking\" and not stalled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spinner_thread.py: the spin and slow functions\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "from threading import Thread, Event\n",
    "\n",
    "def spin(msg: str, done: Event): # <1>\n",
    "    for char in itertools.cycle('|/-\\\\'): # <2>\n",
    "        status = f'\\r{char} {msg}' # <3>\n",
    "        print(status, flush=True, end='')\n",
    "        if done.wait(.1): # <4>\n",
    "            break # <5>\n",
    "    blanks = ' ' * len(status)\n",
    "    print(f'\\r{blanks}\\r', end='') # <6>\n",
    "    print(slow())\n",
    "    \n",
    "def slow():\n",
    "    time.sleep(3) # <7>\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from threading import Thread, Event\n",
    "\n",
    "def spin(msg: str, done: Event): # <1>\n",
    "    for char in itertools.cycle('|/-\\\\'): # <2>\n",
    "        status = f'\\r{char} {msg}' # <3>\n",
    "        print(status, flush=True, end='')\n",
    "        if done.wait(.2): # <4>\n",
    "            break # <5>\n",
    "    blanks = ' ' * len(status)\n",
    "    print(f'\\r{blanks}\\r', end='') # <6> \n",
    "\n",
    "done = Event()        \n",
    "spin('thinking!', done)\n",
    "\n",
    "# You can't make done.wait() return True and break the for loop,\n",
    "# because done.set() is only called after spin() has finished.\n",
    "# You can only call done.set() to make spin() function return \n",
    "# from another thread.\n",
    "done.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This function will run in a separate thread. The `done` argument is an instance of `threading.Event`, a simple object to synchronize threads.\n",
    "\n",
    "2. This is an infinite loop because `itertools.cycle` yields one character at a time, cycling through the string forever.\n",
    "\n",
    "3. The trick for text-mode animation: move the cursor back to the start of the line with the carriage return ASCII control character('\\r')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shubham\n",
      "mishra\n",
      "mishram\n"
     ]
    }
   ],
   "source": [
    "# \\n passes the cursor to beginning of a new line\n",
    "print('shubham\\nmishra')\n",
    "\n",
    "# \\r passes the cursor to the starting point\n",
    "print('shubham\\rmishra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The `Event.wait(timeout=None)` method returns `True` when the event is set by another thread; if the `timeout` elapses, it return `False`. The .1s timeout sets the \"frame rate\" of the animation to 10FPS. if you want the spinner to go faster, use a smaller timeout.\n",
    "\n",
    "5. Exit the infinite loop.\n",
    "\n",
    "6. Clear the status line by overwriting with spaces and moving the cursor back to the beginning.\n",
    "\n",
    "7. `slow()` will be called by the main thread. Imagine this is a slow API call over the network. Calling `sleep` blocks the main thread, but the GIL is released so the spinner thread can proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By design, there is no API for terminating a thread in Python. You must send it a message to shut down.\n",
    "\n",
    "The `threading.Event` class is Python's simplest signalling mechanism to coordinate threads. An `Event` instance has an internal boolean flag that starts as `False`. Calling `Event.set()` sets the flag to `True`. While the flag is false, if a thread calls `Event.wait()`, it is blocked until another thread calls `Event.set()`, at which time `Event.wait()` returns `True`. If a timeout in seconds is given to `Event.wait(s)`, this call returns `False` when the timeout elapses, or return `True` as soon as `Event.set()` is called by another thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spinner object: <Thread(Thread-5 (spin), initial)>\n",
      "42          \n",
      "Answer: 42\n"
     ]
    }
   ],
   "source": [
    "# spinner_thread.py: the supervisor and main functions\n",
    "\n",
    "def supervisor() -> int: #1\n",
    "    done = Event() #2\n",
    "    spinner = Thread(target=spin, args=('thinking!', done)) #3\n",
    "    print(f'spinner object: {spinner}') #4\n",
    "    spinner.start() #5\n",
    "    result = slow() #6\n",
    "    done.set() #7\n",
    "    spinner.join()  #8\n",
    "    return result\n",
    "\n",
    "def main() -> None:\n",
    "    result = supervisor() #9\n",
    "    print(f'Answer: {result}')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `supervisor` will return the result of `slow`.\n",
    "   \n",
    "2. The `threading.Event` instance is the key to coordinate the activities of the `main` thread and the `spinner` thread, as explained further down.\n",
    "\n",
    "3. To create a new `Thread`, provide a function as the `target` keyword argument, and positional arguments to the `target` as a tuple passed via `args`.\n",
    "\n",
    "4. Display the `spinner` object. The output is `<Thread(Thread-1, initial)>`, where `initial` is the state of the thread--meaning it has not started.\n",
    "\n",
    "5. Start the `spinner` thread.\n",
    "\n",
    "6. Call `slow`, which blocks the `main` thread. Meanwhile, the secondary thread is running the spinner animation.\n",
    "\n",
    "7. Set the `Event` flag to `True`; this will terminate the `for` loop inside the `spin` function.\n",
    "\n",
    "8. Wait until the `spinner` thread finishes.\n",
    "\n",
    "9. Run the `supervisor` function. I wrote separate `main` and `supervisor` functions to make this example look more like the `asyncio` version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spinner with Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create a `multiprocessing.Process` instance, a whole new Python interpreter is started as a child process in the background. Since each Python process has its own GIL, this allows your program to use all available CPU cores--but that ultimately depends on the operating system scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spinner object: <Process name='Process-3' parent=648 initial>\n",
      "Answer: 42\n"
     ]
    }
   ],
   "source": [
    "# spinner_proc.py: only the changed parts are shown; \n",
    "# everything else is the same as spinner_thread.py\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "#1 The basic multiprocessing API imitates the threading API,\n",
    "# but type hints and Mypy expose this difference: \n",
    "# multiprocessing.Event is a function(not a class\n",
    "# like threading.Event) which returns a synchronize.Event\n",
    "# instance...\n",
    "from multiprocessing import Process, Event\n",
    "\n",
    "#2 ...forcing us import multiprocessing.synchronize...\n",
    "from multiprocessing import synchronize\n",
    "\n",
    "#3 ...to write this type hind. \n",
    "# spin and slow functions are unchanged.\n",
    "from multiprocessing import synchronize\n",
    "def spin(msg: str, done: synchronize.Event) -> None: \n",
    "    for char in itertools.cycle('|/-\\\\'):\n",
    "        status = f'\\r{char} {msg}' \n",
    "        print(status, flush=True, end='')\n",
    "        if done.wait(.1): \n",
    "            break \n",
    "    blanks = ' ' * len(status)    \n",
    "    print(f'\\r{blanks}\\r', end='') \n",
    "    print(slow())\n",
    "    \n",
    "def slow():\n",
    "    time.sleep(3) \n",
    "    return 42\n",
    "\n",
    "def supervisor() -> int:\n",
    "    done = Event()\n",
    "    \n",
    "    #4 Basic usage of the Process class is similar to Thread.\n",
    "    spinner = Process(target=spin,\n",
    "                        args=('thinking', done))\n",
    "    #5 The spinner object is displayed as <Process\n",
    "    # name='Process-1' parent=14868 initial>, where\n",
    "    # 14868 is the process ID of the Python instance\n",
    "    # running spinner_proc.py.\n",
    "    print(f'spinner object: {spinner}')\n",
    "    spinner.start()\n",
    "    slow()\n",
    "    result = slow()\n",
    "    \n",
    "    done.set()\n",
    "    spinner.join()\n",
    "    return result\n",
    "\n",
    "def main() -> None:\n",
    "    result = supervisor() \n",
    "    print(f'Answer: {result}')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "# Spinner with Process runs without animation. \n",
    "# Animation works in terminal but not in Jupyter Notebook cells.\n",
    "# Why?\n",
    "# Seems no print output is displayed for created process in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic API of threading and multiprocessing are similar, but their implementation is very different, and multiprocessing has a much larger API to handle the added complexity of multiprocess programming. For example objects crossing process boundaries have to be serialized and deserialized, which creates overhead.\n",
    "\n",
    "Since Python 3.8, there's a `multiprocessing.shared_memory` package in the standard library, but is does not support instances of user-defined classes. Besides raw bytes, the package allows processes to share a `ShareableList`, a mutable sequence type that hold a fixed number of items of types `int`, `float`, `bool`, and `None`, as well as `str` and `bytes` up to 10MB per item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spinner with Coroutines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coroutines are driven by an application-level event loop that manages a queue of pending coroutines, drivers them one by one, monitors events triggered by I/O operations initiated by coroutines, and passes control back to the corresponding coroutine when each event happens. The event loop and the library coroutines and the user coroutines all execute in a single thread. Therefore, any time spent in a coroutine slows down the event loop--and all other coroutines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "\n",
    "#1 We don't need the Event argument that was used to\n",
    "# signal that slow had completed its job in spinner_thread.py.\n",
    "async def spin(msg: str) -> None:\n",
    "    for char in itertools.cycle(r'\\|/-'):\n",
    "        status = f'\\r{char} {msg}'\n",
    "        print(status, flush=True, end='')\n",
    "        try: \n",
    "            \n",
    "            #2 Use await asyncio.sleep(.1) instead of done.wait(.1)\n",
    "            # to pause without blocking other coroutines.\n",
    "            await asyncio.sleep(.1)\n",
    "        \n",
    "        #3 asyncio.CancelledError is raised when the cancle\n",
    "        # method is called on the Task controlling this coroutine.\n",
    "        # Time to exit the loop.\n",
    "        except asyncio.CancelledError:\n",
    "            break\n",
    "    blanks = ' ' * len(status)\n",
    "    print(f'\\r{blanks}\\r', end='')\n",
    "    \n",
    "async def slow() -> int:\n",
    "    #4 The slow coroutine also use await asyncio.sleep(.1) instead \n",
    "    # of time.sleep(3)\n",
    "    await asyncio.sleep(3)\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spinner_async.py: the main function and supervisor coroutine\n",
    "\n",
    "# Native coroutines are defined with async def.\n",
    "async def supervisor() -> int:\n",
    "    \n",
    "    # asyncio.create_task schedules the spin coroutine to run,\n",
    "    # and returns a Task object immediately.\n",
    "    spinner = asyncio.create_task(spin('thinking!'))\n",
    "    \n",
    "    # The repr of the spinner object looks like <Task pending\n",
    "    # name='Task-1' coro=<spin() running at \n",
    "    # /path/to/spinner_async.py:11>>\n",
    "    print(f'spinner object: {spinner}')\n",
    "    \n",
    "    # The await keyword calls slow, blocking supervisor until\n",
    "    # slow returns. the return value of slow will be assigned\n",
    "    # to result.\n",
    "    result = await slow()\n",
    "    \n",
    "    # The Task.cancel method raises a CancelledError exception\n",
    "    # inside the spin coroutine.\n",
    "    spinner.cancel()\n",
    "    return result\n",
    "\n",
    "# main is the only regular function defines in this program\n",
    "# --the others are coroutines.\n",
    "def main() -> None:\n",
    "    # The asyncio.run function starts the event loop to drive the\n",
    "    # coroutine that will eventually set the other coroutines in motion.\n",
    "    # The main function will stay blocked until supervisor returns.\n",
    "    # The return value of supervisor will be the return value of \n",
    "    # asyncio.run.\n",
    "    result = asyncio.run(supervisor())\n",
    "    print(f'Answer: {result}')\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three main ways of running a coroutine:\n",
    "\n",
    "- `asyncio.run(coro())`: Called from a regular function to drive a coroutine object that usually is the entry point for all the asynchronous code in a program, like the `supervisor` in this example. This call blocks until the body of coro returns. The return value of the `run()` call is whatever the body of `coro` returns.\n",
    "\n",
    "- `asyncio.create_task(coro())`: Called from a coroutine to schedule another coroutine to execute eventually. This call does not suspend the current coroutine. It returns a `Task` instance, an object that wraps the coroutine object and provides methods to control and query its state.\n",
    "\n",
    "- `await coro()`: Called from a coroutine to transfer control to the coroutine object returned by `coro()`. This suspends the current coroutine until the body of `coro` returns. The value of the await expression is whatever the coroutine object returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't run asyncio.run() in Jupyter Notebook\n",
    "import asyncio\n",
    "\n",
    "async def my_coroutine(msg):\n",
    "    print(\"Coroutine is running...\", msg)\n",
    "    return 42\n",
    "\n",
    "result = asyncio.run(my_coroutine('thinking!'))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never use `time.sleep()` in `asyncio` coroutines unless you want to pause your whole program. If a coroutine needs to spend some time doing nothing, it should `await` on an `asyncio.sleep(DELAY)` call. This yields control back to the `asyncio` event loop, which can drive other pending coroutines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisors Side by Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of the differences and similarities between the coroutine version and thread version of the `supervisor` implementations:\n",
    "\n",
    "- An `asyncio.Task` is roughly equivalent to a `threading.Thread` object. \n",
    "- A `Task` drives a coroutine object, while a `Thread` runs a function.\n",
    "- A coroutine yields control explicitly with the `await` keyword.\n",
    "- You don't instantiate a `Task` directly; you schedule it by calling `asyncio.create_task(coro())`.\n",
    "- When `asyncio.create_task(...)` returns a `Task` object, it is already scheduled to run, but a `Thread` instance must be explicitly started with `Thread.start()`.\n",
    "- In the threaded `supervisor, slow` is a plain function and is directly called by the `main` thread. In the coroutine version, `slow` is a coroutine and must be driven by a `Task` object.\n",
    "- There is no API to terminate a thread from the outside; instead, you must send a signal--like setting the `done Event` object. For tasks, there is the `Task.cancel()` instance method, which raises `CancelledError` at the `await` expression while the coroutine body is currently suspended. You can perform cleanup by catching `CancelledError` in the coroutine body.\n",
    "- The `supervisor` coroutine must be started with `asyncio.run` in the main function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Real Impact of the GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a CPU-intensive code. Consider the function `is_prime` which returns `True` if the argument is a prime number, `False` if it's not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70711463\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# prime.py: an easy to read primality check, \n",
    "# from Python's ProcessPoolExecutor example\n",
    "\n",
    "import math\n",
    "\n",
    "def is_prime(n: int) -> bool:\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0: \n",
    "        return False\n",
    "    \n",
    "    root = math.isqrt(n)\n",
    "    print(root)\n",
    "    for i in range(3, root + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(is_prime(5_000_111_000_222_021))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will happen if we do a replacement?\n",
    "\n",
    "- For multiprocessing: Replace `time.sleep(3)` with a call to `is_prime` in spinner_proc.py. The spinner is controlled by a child process, so it continues to animate while the main process is busy checking if the number is prime.\n",
    "\n",
    "- For threading: Replace `time.sleep(3)` with a call to `is_prime` in spinner_thread.py. The spinner keeps spinning because Python suspends the running thread every 5ms, making the GIL available to other pending threads. \n",
    "\n",
    "- For asyncio: Replace `await asyncio.sleep(3)` with a call to `is_prime` in spinner_asyncio.py. `is_prime` is not a `asyncio` function, so it blocks the event loop. When `is_prime` returns, `slow` returns as well, and `supervisor` cancel the `spinner` task before it is execute even once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def is_prime(n: int) -> bool:\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0: \n",
    "        return False\n",
    "    \n",
    "    root = math.isqrt(n)\n",
    "    print(root)\n",
    "    \n",
    "    for i in range(3, root + 1, 2): \n",
    "        if i%1111113 == 0:\n",
    "            await asyncio.sleep(.05)\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    # await asyncio.sleep(2)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with this `asyncio is_prime`, which call `await asyncio.sleep()` every 100,000 iterations, the spinner was smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Homegrown Process Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 True\n",
      "142702110479723 True\n",
      "299593572317531 True\n",
      "3333333333333301 True\n",
      "3333333333333333 False\n",
      "3333335652092209 False\n",
      "4444444444444423 True\n",
      "4444444444444444 False\n",
      "4444444488888889 False\n",
      "5555553133149889 False\n",
      "5555555555555503 True\n",
      "5555555555555555 False\n",
      "6666666666666666 False\n",
      "6666666666666719 True\n",
      "6666667141414921 False\n",
      "7777777536340681 False\n",
      "7777777777777753 True\n",
      "7777777777777777 False\n",
      "9999999999999917 True\n",
      "9999999999999999 False\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "PRIME_FIXTURE = [\n",
    "    (2, True),\n",
    "    (142702110479723, True),\n",
    "    (299593572317531, True),\n",
    "    (3333333333333301, True),\n",
    "    (3333333333333333, False),\n",
    "    (3333335652092209, False),\n",
    "    (4444444444444423, True),\n",
    "    (4444444444444444, False),\n",
    "    (4444444488888889, False),\n",
    "    (5555553133149889, False),\n",
    "    (5555555555555503, True),\n",
    "    (5555555555555555, False),\n",
    "    (6666666666666666, False),\n",
    "    (6666666666666719, True),\n",
    "    (6666667141414921, False),\n",
    "    (7777777536340681, False),\n",
    "    (7777777777777753, True),\n",
    "    (7777777777777777, False),\n",
    "    (9999999999999917, True),\n",
    "    (9999999999999999, False),\n",
    "]\n",
    "\n",
    "NUMBERS = [n for n, _ in PRIME_FIXTURE]\n",
    "\n",
    "# tag::IS_PRIME[]\n",
    "def is_prime(n: int) -> bool:\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "\n",
    "    root = math.isqrt(n)\n",
    "    for i in range(3, root + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "# end::IS_PRIME[]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for n, prime in PRIME_FIXTURE:\n",
    "        prime_res = is_prime(n)\n",
    "        assert prime_res == prime\n",
    "        print(n, prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 20 numbers sequentially:\n",
      "               2  P  0.000001s\n",
      " 142702110479723  P  0.305576s\n",
      " 299593572317531  P  0.418256s\n",
      "3333333333333301  P  1.395235s\n",
      "3333333333333333     0.000006s\n",
      "3333335652092209     1.350842s\n",
      "4444444444444423  P  1.551151s\n",
      "4444444444444444     0.000001s\n",
      "4444444488888889     1.582557s\n",
      "5555553133149889     1.753850s\n",
      "5555555555555503  P  1.782814s\n",
      "5555555555555555     0.000006s\n",
      "6666666666666666     0.000000s\n",
      "6666666666666719  P  1.939351s\n",
      "6666667141414921     1.925169s\n",
      "7777777536340681     2.036795s\n",
      "7777777777777753  P  2.055988s\n",
      "7777777777777777     0.000007s\n",
      "9999999999999917  P  2.349198s\n",
      "9999999999999999     0.000005s\n",
      "Total time: 20.45s\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Result(NamedTuple):  # <1>\n",
    "    prime: bool\n",
    "    elapsed: float\n",
    "\n",
    "def check(n: int) -> Result:  # <2>\n",
    "    t0 = perf_counter()\n",
    "    prime = is_prime(n)\n",
    "    return Result(prime, perf_counter() - t0)\n",
    "\n",
    "def main() -> None:\n",
    "    print(f'Checking {len(NUMBERS)} numbers sequentially:')\n",
    "    t0 = perf_counter()\n",
    "    for n in NUMBERS:  # <3>\n",
    "        prime, elapsed = check(n)\n",
    "        label = 'P' if prime else ' '\n",
    "        print(f'{n:16}  {label} {elapsed:9.6f}s')\n",
    "\n",
    "    elapsed = perf_counter() - t0  # <4>\n",
    "    print(f'Total time: {elapsed:.2f}s')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing does't work right in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 20 numbers with 16 processes:\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from time import perf_counter\n",
    "from typing import NamedTuple\n",
    "from multiprocessing import Process, SimpleQueue, cpu_count  # <1>\n",
    "from multiprocessing import queues  # <2>\n",
    "\n",
    "\n",
    "class PrimeResult(NamedTuple):  # <3>\n",
    "    n: int\n",
    "    prime: bool\n",
    "    elapsed: float\n",
    "\n",
    "JobQueue = queues.SimpleQueue[int]  # <4>\n",
    "ResultQueue = queues.SimpleQueue[PrimeResult]  # <5>\n",
    "\n",
    "def check(n: int) -> PrimeResult:  # <6>\n",
    "    t0 = perf_counter()\n",
    "    res = is_prime(n)\n",
    "    return PrimeResult(n, res, perf_counter() - t0)\n",
    "\n",
    "def worker(jobs: JobQueue, results: ResultQueue) -> None:  # <7>\n",
    "    while n := jobs.get():  # <8>\n",
    "        results.put(check(n))  # <9>\n",
    "    results.put(PrimeResult(0, False, 0.0))  # <10>\n",
    "\n",
    "def start_jobs(\n",
    "    procs: int, jobs: JobQueue, results: ResultQueue  # <11>\n",
    ") -> None:\n",
    "    for n in NUMBERS:\n",
    "        jobs.put(n)  # <12>\n",
    "    for _ in range(procs):\n",
    "        proc = Process(target=worker, args=(jobs, results))  # <13>\n",
    "        proc.start()  # <14>\n",
    "        jobs.put(0)  # <15>\n",
    "# end::PRIMES_PROC_TOP[]\n",
    "\n",
    "# tag::PRIMES_PROC_MAIN[]\n",
    "def main() -> None:\n",
    "    # if len(sys.argv) < 2:  # <1>\n",
    "    #     procs = cpu_count()\n",
    "    # else:\n",
    "    #     procs = int(sys.argv[1])\n",
    "    \n",
    "    procs = cpu_count()\n",
    "\n",
    "    print(f'Checking {len(NUMBERS)} numbers with {procs} processes:')\n",
    "    t0 = perf_counter()\n",
    "    jobs: JobQueue = SimpleQueue()  # <2>\n",
    "    results: ResultQueue = SimpleQueue()\n",
    "    start_jobs(procs, jobs, results)  # <3>\n",
    "    checked = report(procs, results)  # <4>\n",
    "    elapsed = perf_counter() - t0\n",
    "    print(f'{checked} checks in {elapsed:.2f}s')  # <5>\n",
    "\n",
    "def report(procs: int, results: ResultQueue) -> int: # <6>\n",
    "    checked = 0\n",
    "    procs_done = 0\n",
    "    while procs_done < procs:  # <7>\n",
    "        n, prime, elapsed = results.get()  # <8>\n",
    "        if n == 0:  # <9>\n",
    "            procs_done += 1\n",
    "        else:\n",
    "            checked += 1  # <10>\n",
    "            label = 'P' if prime else ' '\n",
    "            print(f'{n:16}  {label} {elapsed:9.6f}s')\n",
    "    return checked\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "# end::PRIMES_PROC_MAIN[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python in the Multi-Core World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software\n",
    ">\n",
    "> -- Herb Sutter, March 2005\n",
    ">\n",
    "> The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have run out of room with most of their traditional approaches to boosting CPU performance. Instead of driving clock speeds and straight-line instruction throughput ever higher, they are instead turning en masse to hype-threading and multi-core architectures. The result is that the free lunch of CPU performance scaling is over, and the only way to get more performance is to write concurrent code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GIL make the Python interpreter faster when running on a single core, and its implementation simpler. The GIL also makes it easier to write simple extensions through the Python/C API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Administration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is widely used to manage large fleets of servers, routers, load balancers, and network-attached storage(NAS). It's also a leading option in a software-defined networking(SDN) and ethical hacking. Major cloud service providers support Python through libraries and tutorials authored by the providers themselves or by their large communities of Python users.\n",
    "\n",
    "Beyond the standard library, there are popular Python-based projects to manage server clusters: tools like `Ansible` and `Salt`, as well as libraries like `Fabric`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science--including artificial intelligence--and scientific computing are very well served by Python.\n",
    "\n",
    "Python's data science ecosystem includes impressive tools such as:\n",
    "\n",
    "- Project `Jupyter`\n",
    "- `TensorFlow` and `Pytorch`\n",
    "- `Dask`: A parallel computing library that can farm out work to local processes or clusters of machines, \"tested on some of the largest supercomputers in the world\"--as their home page states. Dask offers API that closely emulate `NumPy`, `pandas` and `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server-side Web/Mobile Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is widely used in web application and for the backend APIs supporting mobile applications. \n",
    "\n",
    "Here are some components in Python engagements:\n",
    "\n",
    "- Application caches: `memcached`, `Redis`, `Varnish`\n",
    "- Relational databases: `PostgresSQL`, `MySQL`\n",
    "- Document databases: `Apache CouchDB`, `MongoDB`\n",
    "- Full-text indexes: `Elasticsearch`, `Apache Solr`\n",
    "- Message queue: `RabbitMQ`, `Redis`\n",
    "\n",
    "For Python Server-side applications, two specific components are often deployed:\n",
    "- An application server to distribute the load among several instances of Python application. \n",
    "- A task queue built around the message queue, providing a higher-level, easier-to-use API to distribute tasks to processes running on other machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Server architecture](./img/2023-11-02-23-05-58.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WSGI Application Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WSGI--the Web Server Gateway Interface--is a standard API for a Python framework or application to receive requests from an HTTP server and send responses to it. WSGI application serves one or more processes running your application, maximizing the use of the available CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WSGI deployment](./img/2023-11-02-23-07-37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-known application servers in Python web projects are:\n",
    "\n",
    "- mod_wsgi\n",
    "- uWSGI\n",
    "- Gunicorn\n",
    "- NGINX Unit\n",
    "\n",
    "For users of the Apache HTTP server, `mod_wsgi` is the best option. It's easy to configure and is suitable for use in Docker containers.\n",
    "\n",
    "uWSGI and Gunicorn are the top choices in 2021. Both are often used with the NGINX HTTP server.\n",
    "\n",
    "It's possible to earn a living as a Python web developer without ever studying the threading, multiprocessing, or asyncio modules: the application server handles concurrency transparently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Task Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed task queues are designed to solve the some requests that may take longer time--for example, sending email or generating a PDF.\n",
    "\n",
    "`Celery` and `RQ` are the best known open source task queues with Python APIs. Cloud provider also offer their own proprietary task queues.\n",
    "\n",
    "These products wrap a message queue and offer a high-level API for delegating tasks to workers, possibly running on different machines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
